#!/usr/bin/env python3
import datetime
import argparse
import logging
import os
import re
import socket
import sys
from logging.handlers import RotatingFileHandler

from typing import List

import configargparse
import htcondor
import numpy as np
from htcondor import JobEventType as jet
from rich import print as rprint, box
from rich.progress import track, Progress, Table
from plotille import Figure


log_inf_list = List[dict]
list_of_logs = List[str]
date_time = datetime.datetime
timedelta = datetime.timedelta

"""

This script is basically reading information from HTCondor log files,
by using the htcondor module
and stores it into dictionaries.
By that it's simple to analyse the data.

The script makes the information compact and easy to under stand,
that's the reason for the name htcompact

Single logs can be read quite easily,
but also it's possible to summarize a whole directory with logs
to see for ex. the average runtime and usage of all the logs

"""

# Exit Codes
"""
 Normal Termination: 0
 No given files: 1
 Wrong Options or Arguments: 2
 TypeError: 3
 Keyboard interruption: 4
"""

# global variables
ALLOWED_MODES = {"a": "analyse",
                 "s": "summarize",
                 "as": "analysed-summary",
                 "d": "default"}

ALLOWED_SHOW_VALUES = ["std-err", "std-out"]
ALLOWED_IGNORE_VALUES = ["execution-details", "times", "host-nodes",
                         "used-resources", "requested-resources",
                         "allocated-resources", "all-resources",
                         "errors"]


# class to store and change global variables
class GlobalPlayer(object):

    def __init__(self):
        self.reset()

    def reset(self):

        # thresholds for bad and low usage of resources
        self.tolerated_usage_threshold = 0.1
        self.bad_usage_threshold = 0.25

        self.store_dns_lookups = dict()
        self.reverse_dns_lookup = False

        # global variables with default values for err/log/out files
        self.std_log = ""
        self.std_err = ".err"
        self.std_out = ".out"

        # redirection tools
        self.redirecting_stdout = None
        self.reading_stdin = None
        self.stdin_input = None

    def check_for_redirection(self):
        """
        This method should be activated first, when output is generated
        It changes the global variables of GlobalServant,
        if stdin or stdout is set

        This is useful to escape color sequences,
        or to avoid user input
        :return:
        """

        self.redirecting_stdout = not sys.stdout.isatty()
        self.reading_stdin = not sys.stdin.isatty()

        if self.reading_stdin:
            self.stdin_input = sys.stdin.readlines()

    def set_std_files(self, std_log=None, std_err=None, std_out=None):
        """
        Set global varibles for standard HTCondor log, error and output files
        adds the dot if missing and string not empty like:
        std_log = log -> .log

        :param std_log: str
        :param std_err: str
        :param std_out: str
        :return:
        """
        if std_log is not None:
            if not std_log.startswith('.') and std_log.__ne__(""):
                std_log = '.' + std_log
            self.std_log = std_log
        if std_err is not None:
            if not std_err.startswith('.') and std_err.__ne__(""):
                std_err = '.' + std_err
            self.std_err = std_err
        if std_out is not None:
            if not std_out.startswith('.') and std_out.__ne__(""):
                std_out = '.' + std_out
            self.std_out = std_out

    def set_thresholds(self, tolerated_usage=None, bad_usage=None):
        if isinstance(tolerated_usage, float):
            self.tolerated_usage_threshold = tolerated_usage
        if isinstance(bad_usage, float):
            self.bad_usage_threshold = bad_usage

    def reverse_dns(self, do_that):
        if isinstance(do_that, bool):
            self.reverse_dns_lookup = do_that

    def manage_thresholds(self, resources: dict) -> dict:
        """

            The important part is that the keywords "Usage", "Requested" exists
            and that at least 3 values are given: cpu, disk, memory

        :param resources:
        :return:
        """

        resources.update(Usage=list(
            resources["Usage"]))  # change to list, to avoid numpy type errors
        for i in range(len(resources['Resources'])):
            # thresholds used vs. requested
            if float(resources['Requested'][i]) != 0:

                deviation = float(resources['Usage'][i]) / float(
                    resources['Requested'][i])

                # color red if more than bad_usage_thresholds % away
                # from the requested value
                if deviation >= 1 + self.bad_usage_threshold \
                        or deviation <= 1 - self.bad_usage_threshold:
                    resources['Usage'][i] = f"[red]" \
                        f"{str(resources['Usage'][i])}[/red]"

                # color yellow if more than low_usage_threhold % away
                # from requested value
                elif deviation >= 1 + self.tolerated_usage_threshold \
                        or deviation <= 1 - self.tolerated_usage_threshold:
                    resources['Usage'][i] = f"[yellow]" \
                        f"{str(resources['Usage'][i])}[/yellow]"
                # if nan, mark yellow
                elif str(resources['Usage'][i]) == "nan":
                    resources['Usage'][i] = f"[yellow2]" \
                        f"{str(resources['Usage'][i])}[/yellow2]"
                # else it's okay, color green,
                else:
                    resources['Usage'][i] = f"[green]" \
                        f"{str(resources['Usage'][i])}[/green]"

        return resources


###############################
GlobalServant = GlobalPlayer()  # define global variables
###############################


class CustomFormatter(argparse.HelpFormatter):
    """Custom formatter for setting argparse formatter_class. Identical to the
    default formatter, except that very long option strings are split into two
    lines.

    Solution discussed on: https://bit.ly/32CkCWK
    """

    def _format_action_invocation(self, action):
        if not action.option_strings:
            metavar, = self._metavar_formatter(action, action.dest)(1)
            return metavar
        else:
            parts = []
            # if the Optional doesn't take a value, format is:
            #    -s, --long
            if action.nargs == 0:
                parts.extend(action.option_strings)
            # if the Optional takes a value, format is:
            #    -s ARGS, --long ARGS
            else:
                default = action.dest.upper()
                args_string = self._format_args(action, default)
                for option_string in action.option_strings:
                    parts.append('%s %s' % (option_string, args_string))
            if sum(len(s) for s in parts) < self._width - (len(parts) - 1) * 2:
                return ', '.join(parts)
            else:
                return ',\n  '.join(parts)


# parser for prioritised flags
def setup_prioritized_parser():
    """
    Prioritized options
    :return:
    """
    parser = argparse.ArgumentParser(add_help=False)
    parser.add_argument("--no-config",
                        action="store_true")
    parser.add_argument("-c", "--config", nargs=1)
    parser.add_argument("--version",
                        help="Print out extended execution details",
                        action="store_true")

    parser.add_argument("-f", "--files",
                        nargs=1,
                        action="append",
                        dest="more_files",
                        default=[],
                        help="ONE path to log file")
    return parser


def setup_commandline_parser(default_config_files=[])\
        -> configargparse.ArgumentParser:
    """
    Defines parser with all arguments listed below
    :param default_config_files: list with config file hierarchy to look for
    :return:
    """

    parser = configargparse.\
        ArgumentParser(formatter_class=CustomFormatter,
                       default_config_files=default_config_files)

    parser.add_argument("path",
                        nargs="*",
                        action="append",
                        default=[],
                        help="ANY number of paths to log file(s)")
    # also to add files with different destination,
    # to be used for config file / escaping flags with action=append
    parser.add_argument("-f", "--files",
                        nargs=1,
                        action="append",
                        dest="more_files",
                        default=[],
                        help="ONE path to log file")
    parser.add_argument("-r", "--recursive",
                        action="store_true",
                        default=None,
                        help="Recursive search through directory hierarchy")

    parser.add_argument("--version",
                        help="Print out extended execution details",
                        action="store_true")

    parser.add_argument("-v", "--verbose",
                        help="Print out extended execution details",
                        action="store_true",
                        default=None)
    parser.add_argument("--generate-log-file",
                        nargs="?",
                        const="htcompact.log",
                        default=None,
                        help="generates output about the process,"
                             " which is mostly useful for developers, "
                             "if no file is specified, default: htcompact.log")

    all_vals = []
    for item in ALLOWED_MODES.items():
        all_vals.extend(list(item))

    parser.add_argument("-m", "--mode",
                        help="Specifiy an interpretation mode",
                        choices=all_vals)

    parser.add_argument("-s", dest="summarizer_mode",
                        help="Short for --mode summarize,"
                             " combine with -a for analysed-summary mode",
                        action="store_true")

    parser.add_argument("-a", dest="analyser_mode",
                        help="Short for --mode analyse,"
                             " combine with -s for analysed-summary mode",
                        action="store_true")

    parser.add_argument("--std-log",
                        help="Specify the log file suffix",
                        type=str)
    parser.add_argument("--std-out",
                        help="Specify the output file suffix",
                        type=str)
    parser.add_argument("--std-err",
                        help="Specify the error file suffix",
                        type=str)

    ignore_metavar = "{" + ALLOWED_IGNORE_VALUES[0] + " ... " \
                     + ALLOWED_IGNORE_VALUES[-1] + "}"
    allowed_ign_vals = ALLOWED_IGNORE_VALUES[:]  # copying
    allowed_ign_vals.append('')  # needed so empty list are valid in config
    parser.add_argument("--ignore",
                        nargs="+",
                        action="append",
                        choices=allowed_ign_vals,
                        metavar=ignore_metavar,
                        dest="ignore_list",
                        default=[],
                        help="Ignore a section to not be printed")
    allowed_show_vals = ALLOWED_SHOW_VALUES[:]  # copying
    allowed_show_vals.append('')  # needed so empty list are valid in config
    parser.add_argument("--show",
                        nargs="+",
                        action="append",
                        dest="show_list",
                        choices=allowed_show_vals,
                        default=[],
                        help="Show more details")

    parser.add_argument("--filter",
                        nargs="+",
                        metavar="keywords",
                        help="Filter for the given keywords",
                        default=[],
                        dest="filter_keywords",
                        action="append",
                        type=str)
    parser.add_argument("--extend",
                        action="store_true",
                        dest="filter_extended",
                        default=None,
                        help="extend the filter keyword list "
                             "by specific error keywords")

    parser.add_argument("--reverse-dns-lookup",
                        action="store_true",
                        default=None,
                        help="Resolve the ip-address of an execution nodes"
                             " to their dns entry")

    parser.add_argument("--tolerated-usage",
                        type=float,
                        help="Threshold to warn the user, "
                             "when a given percentage is exceeded "
                             "between used and requested resources")

    parser.add_argument("--bad-usage",
                        type=float,
                        help="Threshold to signal overuse/waste of resources, "
                             "when a given percentage is exceeded "
                             "between used and requested resources")
    parser.add_argument("-c", "--config",
                        is_config_file=True,
                        help="ONE path to config file")
    parser.add_argument("--no-config",
                        action="store_true",
                        help="Do not search for config")
    parser.add_argument("--save-config",
                        is_write_out_config_file_arg=True,
                        help="Stores the current configuration"
                             " into a config file")

    return parser


def manage_params(args: list) -> dict:
    """
    Uses the argparse parser defined in define_parser()
    returns a dict looking like (default):

     {'verbose': False,
      'generate_log_file': None,
      'mode': None,
      'std_log': '',
      'std_out': '.out',
      'std_err': '.err',
      'ignore_list': [],
      'show_list': [],
      'no_config': False,
      'filter_keywords': [],
      'extend': False,
      'reverse_dns_lookup': False
      'files': []
      ....
      }


    :param args: list of args
    :return: dict with params
    """

    # listen to stdin and add these files
    if GlobalServant.reading_stdin:
        logging.debug("Listening to arguments from stdin")
        for line in GlobalServant.stdin_input:
            args.append(line.rstrip('\n'))

    prio_parsed, args = setup_prioritized_parser().parse_known_args(args)
    # first of all check for prioritised/exit params
    if prio_parsed.version:
        print("Version: v1.2.3")
        sys.exit(0)

    # get files from prio_parsed
    files_list = list()
    for li in prio_parsed.more_files:
        files_list.extend(li)

    if prio_parsed.config:
        # do not use config files if --no-config flag is set
        if prio_parsed.no_config:
            # if as well config is set, exit, because of conflict
            print("htcompact: error: conflict between"
                  " --no-config and --config")
            sys.exit(2)
        # else add config again
        args.extend(["--config", prio_parsed.config[0]])

    # parse config file if not --no-config is set, might change nothing
    if not prio_parsed.no_config:
        config_paths = ['/etc/htcompact.conf',
                        '~/.config/htcompact/htcompact.conf',
                        sys.prefix + '/config/htcompact.conf']
        cmd_parser = setup_commandline_parser(config_paths)
        commands_parsed = cmd_parser.parse_args(args)
        cmd_dict = vars(commands_parsed).copy()

        # extend files list by given paths
        for li in commands_parsed.path:
            files_list.extend(li)
        # add files, if none are given by terminal
        if len(files_list) == 0:
            for li in cmd_dict["more_files"]:
                # make sure that empty strings are not getting inserted
                if len(li) == 1 and li[0] != "":
                    files_list.extend(li)

        # TODO: generate_log_file = True -> convert to bool

        # remove empty string from lists, because configargparse
        # inserts empty strings, when list is empty
        for val in cmd_dict.keys():
            if isinstance(cmd_dict[val], list):
                for li in cmd_dict[val]:
                    if len(li) == 1 and li[0] == "":
                        li.remove("")

    else:
        cmd_parser = setup_commandline_parser()
        commands_parsed = cmd_parser.parse_args(args)
        # extend files list by given paths
        for li in commands_parsed.path:
            files_list.extend(li)
        cmd_dict = vars(commands_parsed).copy()

    del cmd_dict["path"]
    del cmd_dict["more_files"]
    cmd_dict["files"] = files_list

    # concat ignore list
    new_ignore_list = list()
    for li in cmd_dict["ignore_list"]:
        new_ignore_list.extend(li)
    cmd_dict["ignore_list"] = new_ignore_list

    # concat show list
    new_show_list = list()
    for li in cmd_dict["show_list"]:
        new_show_list.extend(li)
    cmd_dict["show_list"] = new_show_list

    # concat filter list
    new_filter_list = list()
    for li in cmd_dict["filter_keywords"]:
        new_filter_list.extend(li)
    cmd_dict["filter_keywords"] = new_filter_list

    # parse the mode correctly
    if commands_parsed.analyser_mode and commands_parsed.summarizer_mode:
        mode = "analysed-summary"
    elif commands_parsed.analyser_mode:
        mode = "analyse"
    elif commands_parsed.summarizer_mode:
        mode = "summarize"
    elif commands_parsed.mode is not None:
        if commands_parsed.mode in ALLOWED_MODES.keys():
            mode = ALLOWED_MODES[commands_parsed.mode]
        else:
            mode = commands_parsed.mode
    else:
        mode = None  # will result in default mode

    cmd_dict["mode"] = mode
    # error handling
    try:
        if cmd_dict["filter_extended"] and\
                len(cmd_dict["filter_keywords"]) == 0:
            raise_value_error("--extend not allowed without --filter")
        if len(cmd_dict["show_list"]) > 0:
            if mode == "analysed-summary" or mode == "summarize":
                raise_value_error("--show only allowed"
                                  " with default and analyser mode")
    except ValueError as err:
        print("htcompact: error: " + str(err))
        sys.exit(2)

    # set global variables for log/err/out files
    GlobalServant.set_std_files(cmd_dict["std_log"],
                                cmd_dict["std_err"],
                                cmd_dict["std_out"])
    GlobalServant.reverse_dns(cmd_dict["reverse_dns_lookup"])
    GlobalServant.set_thresholds(cmd_dict["tolerated_usage"],
                                 cmd_dict["bad_usage"])

    # delete unnecessary information
    del cmd_dict["summarizer_mode"]
    del cmd_dict["analyser_mode"]
    del cmd_dict["version"]
    del cmd_dict["no_config"]

    if cmd_dict['std_log'] is None:
        cmd_dict['std_log'] = ""
    if cmd_dict['std_err'] is None:
        cmd_dict['std_err'] = ".err"
    if cmd_dict['std_out'] is None:
        cmd_dict['std_out'] = ".out"

    # Todo
    # if cmd_dict['save_config'] is not None:
    #     config_name = cmd_dict['save_config']
    #     del cmd_dict["save_config"]
    #     for key, value in dict(cmd_dict).items():
    #         if value is None:
    #             del cmd_dict[key]
    #     new_nsp = argparse.Namespace(**cmd_dict)
    #     print(commands_parsed)
    #     print(new_nsp)
    #     print(cmd_dict)
    #     cmd_parser.write_config_file(new_nsp, [config_name], True)
    return cmd_dict


def raise_value_error(message: str) -> ValueError:
    raise ValueError(message)


def raise_type_error(message: str) -> TypeError:
    raise TypeError(message)


# Todo: time differences over end of year
def gen_time_dict(
        submission_date: date_time,
        execution_date: date_time = None,
        termination_date: date_time = None
) -> (timedelta, timedelta, timedelta):
    """
    Takes in three dates, at least one must be given,
    return the datetime.timedelta objects,
    Depending on the given arguments,
    this function will try to find out the time differences between the events.
    If not all three values are given, it will return None

    :param submission_date: Job Sumbission date from the user
    :param execution_date: The date when the job actually started executing
    :param termination_date: The date when the job was finished
    :return: (waiting_time, runtime, total_time)

    """
    waiting_time = None
    runtime = None
    total_time = None
    today = datetime.datetime.now()
    today = today.replace(microsecond=0)  # remove unnecessary microseconds

    time_desc = list()
    time_vals = list()
    running_over_neyear = False

    # calculate the time difference to last year,
    # if the date is higher that today of running jobs
    # this means the execution started before newyear
    if termination_date is None:
        if submission_date and submission_date > today:
            running_over_neyear = True
            submission_date = submission_date.replace(
                year=submission_date.year - 1)
        if execution_date and execution_date > today:
            running_over_neyear = True
            execution_date = execution_date.replace(
                year=execution_date.year - 1)

    if execution_date and submission_date:
        execution_date = execution_date
        # new year ?
        if submission_date > execution_date:
            running_over_neyear = True
            submission_date = submission_date.replace(
                year=submission_date.year - 1)
        waiting_time = execution_date - submission_date
    if termination_date:
        if waiting_time:
            pass
        if execution_date:
            # new year ?
            if execution_date > termination_date:
                running_over_neyear = True
                execution_date = execution_date.replace(
                    year=execution_date.year - 1)
            runtime = termination_date - execution_date
        if submission_date:
            # new year ?
            if submission_date > termination_date:
                running_over_neyear = True
                submission_date = submission_date.replace(
                    year=submission_date.year - 1)
            total_time = termination_date - submission_date
    # Process still running
    elif waiting_time:
        runtime = today - execution_date
    # Still waiting for execution
    elif submission_date:
        waiting_time = today - submission_date

    # now after collecting all available values try to produce a dict
    # if new year was hitted by one of them, show the year as well
    if running_over_neyear:
        if submission_date:
            time_desc.append("Submission date")
            time_vals.append(submission_date)
        if execution_date:
            time_desc.append("Execution date")
            time_vals.append(execution_date)
        if termination_date:
            time_desc.append("Termination date")
            time_vals.append(termination_date)
    else:
        if submission_date:
            time_desc.append("Submission date")
            time_vals.append(submission_date.strftime("%m/%d %H:%M:%S"))
        if execution_date:
            time_desc.append("Execution date")
            time_vals.append(execution_date.strftime("%m/%d %H:%M:%S"))
        if termination_date:
            time_desc.append("Termination date")
            time_vals.append(termination_date.strftime("%m/%d %H:%M:%S"))

    if waiting_time:
        time_desc.append("Waiting time")
        time_vals.append(waiting_time)
    if runtime:
        time_desc.append("Execution runtime")
        time_vals.append(runtime)
    if total_time:
        time_desc.append("Total runtime")
        time_vals.append(total_time)

    time_dict = {
        "Dates and times": time_desc,
        "Values": time_vals
    }

    return time_dict


def log_to_dict(file: str, sec: int = 0) -> (dict, dict, dict, dict, dict):
    """
    Read the log file with the htcondor module.
    Return five dicts holding information about:
    execution node, used resources, times, used ram history, errors .

    :type file: str
    :param file: HTCondor log file
    :param sec: seconds to wait for new events
    :return: job_dict, res_dict, time_dict, ram_history, errors

    Consider that the return values can be None or empty dictionarys
    """
    job_events = list()
    res_dict = dict()
    time_dict = {
        "Submission date": None,
        "Execution date": None,
        "Termination date": None
    }
    ram_history = list()
    occurred_errors = list()

    has_terminated = False
    invalid_file = False

    try:
        jel = htcondor.JobEventLog(file)
        # Read all currently-available events
        # waiting for 'sec' seconds for the next event.
        for event in jel.events(sec):
            event_type_number = event.get('EventTypeNumber')
            # convert time to datetime object
            date = datetime.datetime.strptime(event.get('EventTime'),
                                              "%Y-%m-%dT%H:%M:%S")
            # update submit date, submission host
            if event.type == jet.SUBMIT:
                time_dict["Submission date"] = date

                match_from_host = re.match(r"<(.+):[0-9]+\?(.*)>",
                                           event.get('SubmitHost'))
                if match_from_host:
                    submitted_host = match_from_host[1]
                    job_events.append(('Submitted from', submitted_host))
                # ERROR
                else:
                    invalid_file = True
                    reason = "Can't read user address"
                    occurred_errors.append(
                        [event_type_number, "Now", "invalid user address",
                         reason])
                    job_events.append(('Submitted from', "invalid user"))
                    raise_value_error("Wrong submission host: " + file)

            # update execution date, execution node
            if event.type == jet.EXECUTE:
                time_dict["Execution date"] = date

                match_to_host = re.match(r"<(.+):[0-9]+\?(.*)>",
                                         event.get('ExecuteHost'))
                if match_to_host:
                    execution_host = match_to_host[1]
                    if GlobalServant.reverse_dns_lookup:  # resolve ip to dns
                        execution_host = gethostbyaddr(execution_host)

                    job_events.append(('Executing on', execution_host))
                # ERROR
                else:
                    invalid_file = True
                    reason = "Can't read host address"
                    occurred_errors.append(
                        [event_type_number, "Now", "invalid host address",
                         reason])
                    job_events.append(('Executing on', "invalid host"))
                    raise_value_error("Wrong execution host: " + file)

            # update ram history dict
            if event.type == jet.IMAGE_SIZE:
                size_update = event.get('Size')
                memory_usage = event.get('MemoryUsage')
                resident_set_size = event.get('ResidentSetSize')
                ram_history.append((date, size_update, memory_usage,
                                    resident_set_size))

            # update resource dict and termination date
            if event.type == jet.JOB_TERMINATED:
                has_terminated = True
                time_dict["Termination date"] = date

                # get all resources, replace by np.nan if value is None
                cpu_usage = event.get('CpusUsage') if event.get(
                    'CpusUsage') is not None else np.nan
                cpu_requested = event.get('RequestCpus') if event.get(
                    'RequestCpus') is not None else np.nan
                cpu_allocated = event.get('Cpus') if event.get(
                    'Cpus') is not None else np.nan
                disk_usage = event.get('DiskUsage') if event.get(
                    'DiskUsage') is not None else np.nan
                disk_requested = event.get('RequestDisk') if event.get(
                    'RequestDisk') is not None else np.nan
                disk_allocated = event.get("Disk") if event.get(
                    'Disk') is not None else np.nan
                memory_usage = event.get('MemoryUsage') if event.get(
                    'MemoryUSage') is not None else np.nan
                memory_requested = event.get('RequestMemory') if event.get(
                    'RequestMemory') is not None else np.nan
                memory_allocated = event.get('Memory') if event.get(
                    'Memory') is not None else np.nan

                # put the data in the dict
                res_dict = {
                    "Resources": ["Cpu", "Disk", "Memory"],
                    "Usage": np.array([cpu_usage, disk_usage, memory_usage],
                                      dtype=float),
                    "Requested": np.array(
                        [cpu_requested, disk_requested, memory_requested],
                        dtype=float),
                    "Allocated": np.array(
                        [cpu_allocated, disk_allocated, memory_allocated],
                        dtype=float)
                }
                normal_termination = event.get('TerminatedNormally')
                # differentiate between normal and abnormal termination
                if normal_termination:
                    job_events.insert(0, ("Termination State",
                                          "[green]Normal[/green]"))
                    return_value = event.get('ReturnValue')
                    job_events.append(("Return Value", return_value))
                else:
                    job_events.insert(0, ("Termination State",
                                          "[red]Abnormal[/red]"))
                    signal = event.get('TerminatedBySignal')
                    job_events.append(("Terminated by Signal", signal))

            # update error dict and termination date
            if event.type == jet.JOB_ABORTED:
                has_terminated = True
                time_dict["Termination date"] = date

                reason = event.get('Reason')
                occurred_errors.append(
                    [event_type_number, date.strftime("%m/%d %H:%M:%S"),
                     "Aborted", reason])
                job_events.insert(0, ("Process was", "[red]Aborted[/red]"))

            # update error dict
            if event.type == jet.JOB_HELD:
                reason = event.get('HoldReason')
                occurred_errors.append(
                    [event_type_number, date.strftime("%m/%d %H:%M:%S"),
                     "JOB_HELD", reason])

            # update error dict
            if event.type == jet.SHADOW_EXCEPTION:
                reason = event.get('Message')
                occurred_errors.append((event_type_number,
                                        date.strftime("%m/%d %H:%M:%S"),
                                        "SHADOW_EXCEPTION", reason))
        else:
            # End of the file
            pass

    except OSError as err:
        invalid_file = True
        if err.args[0] == "ULOG_RD_ERROR":
            rprint(f"[red]{err}: {file}[/red]")
            reason = "Error while reading log file. " \
                     "File was manipulated or contains gpu usage."
            occurred_errors.append(["None", "Now", "ULOG_RD_ERROR", reason])
        else:
            rprint(f"[red]Not able to open the file: {file}[/red]")
    except ValueError as err:
        logging.exception(err)
        rprint(f"[red]{err.__class__.__name__}: {err}[/red]")
    except TypeError as err:
        logging.exception(err)
        rprint(f"[red]{err.__class__.__name__}: {err}[/red]")

    # generate a better time dict
    time_values = list(time_dict.values())
    better_time_dict = gen_time_dict(*time_values)

    # Job still running and file valid
    if not invalid_file and not has_terminated:
        if "Total runtime" in better_time_dict["Dates and times"]:
            rprint(
                "[red]This is not supposed to happen, check your code[/red]")
        elif "Execution runtime" in better_time_dict["Dates and times"]:
            state = "Executing"
        elif "Waiting time" in better_time_dict["Dates and times"]:
            state = "Waiting"
        else:
            state = "Unknown"
        job_events.insert(0, ("Process is", f"[blue]{state}[/blue]"))
    # file not fully readable
    elif invalid_file:
        better_time_dict = dict()  # times have no meaning here
        job_events.insert(0, ("Error", "[red]Error while reading[/red]"))

    job_events_dict = dict()
    error_dict = dict()
    ram_history_dict = dict()
    # convert job_events to a nice and simple dictionary
    if len(job_events) > 0:
        desc, val = zip(*job_events)
        job_events_dict = {
            "Execution details": desc,
            "Values": val
        }

    # convert errors into a dictionary
    if len(occurred_errors) > 0:
        event_numbers, time_list, errors, reasons = zip(*occurred_errors)
        error_dict = {
            "Event Number": list(event_numbers),
            "Time": list(time_list),
            "Error": list(errors),
            "Reason": list(reasons)
        }
    # convert ram_history to a dictionary
    if len(ram_history) > 0:
        time_list, img_size, mem_usage, res_set_size = zip(*ram_history)
        ram_history_dict = {
            "Dates": list(time_list),
            "Image size updates": list(img_size),
            "Memory usages": list(mem_usage),
            "Resident Set Sizes": list(res_set_size)
        }

    return (job_events_dict,
            res_dict,
            better_time_dict,
            ram_history_dict,
            error_dict)


def gethostbyaddr(ip):
    """
        this function is supposed to filter a given
         ip for it's representative domain name like google.com
        :return: resolved domain name, else give back the ip
    """
    try:
        if ip in list(GlobalServant.store_dns_lookups.keys()):
            return GlobalServant.store_dns_lookups[ip]
        # else lookup
        reversed_dns = socket.gethostbyaddr(ip)
        logging.debug(
            'Lookup successful ' + ip + ' resolved as: ' + reversed_dns[0])
        # store
        GlobalServant.store_dns_lookups[ip] = reversed_dns[0]
        # return
        return reversed_dns[0]
    except Exception:
        logging.debug('Not able to resolve the IP: ' + ip)
        # also store
        GlobalServant.store_dns_lookups[ip] = ip
        return ip


def htcondor_stderr(file: str, std_err=".err") -> str:
    """

    :param file: HTCondor stderr file
    :param std_err: stderr suffix, default: .err
    :return: filtered content
    """

    # accept files without the std_err suffix
    if std_err.__ne__("") and file[-len(std_err):].__eq__(std_err):
        job_spec_id = file[:-len(std_err)]
    elif std_err.__ne__(""):
        job_spec_id = file
    else:
        raise_value_error("Missing specification for the std_err")

    output_string = ""
    path = job_spec_id+std_err
    try:
        if os.path.getsize(path) == 0:
            return ""

        with open(path, "r") as error_content:
            for line in error_content:
                line = line.strip("\n")
                if "err" in line.lower():
                    output_string += f"[red]{line}[/red]\n"
                elif "warn" in line.lower():
                    output_string += f"[yellow]{line}[/yellow]\n"

    except NameError as err:
        logging.exception(err)
        rprint("[red]The smart_output_error method requires a " +
               std_err + " file as parameter[/red]")
    except FileNotFoundError:
        relevant = file.split("/")[-2:]
        match = re.match(r".*?([0-9]{3,}_[0-9]+)" + std_err, relevant[1])
        rprint(
            f"[yellow]There is no related {std_err} file:"
            f" {relevant[1]} in the directory:\n[/yellow]"
            f"[cyan]'{os.path.abspath(relevant[0])}'\n"
            f" with the prefix: {match[1]}[/cyan]")
    except TypeError as err:
        logging.exception(err)
        # we continue here no exit needed
    finally:
        return output_string


def htcondor_stdout(file: str, std_out=".out") -> str:
    """
    :param: HTCondor stdout file
    :param std_out: stdout suffix, default: .out
    :return: content
    """

    # accept files without the std_out suffix
    if std_out.__ne__("") and file[-len(std_out):].__eq__(std_out):
        job_spec_id = file[:-len(std_out)]
    elif std_out.__ne__(""):
        job_spec_id = file
    else:
        raise_value_error("Missing specification for the std_out file")

    output_string = ""
    path = job_spec_id + std_out
    try:

        if os.path.getsize(path) == 0:
            return ""

        with open(path, "r") as output_content:
            output_string += "".join(output_content.readlines())
    except NameError as err:
        logging.exception(err)
        rprint(
            "[red]The smart_output_output method requires a " +
            std_out + " file as parameter[/red]")
    except FileNotFoundError:
        relevant = file.split("/")[-2:]
        match = re.match(r".*?([0-9]{3,}_[0-9]+)" + std_out, relevant[1])
        rprint(
            f"[yellow]There is no related {std_out}"
            f" file: {relevant[1]} in the directory:\n"
            f"[/yellow]"
            f"[cyan]'{os.path.abspath(relevant[0])}'\n"
            f" with the prefix: {match[1]}[/cyan]")
    except TypeError as err:
        logging.exception(err)
        # we continue here no exit needed
    finally:

        return output_string


def get_job_spec_id(file: str, std_log):
    """
    Insert a HTCondor file and the suffix
    and cut of the suffix to get just the job specification id

    Example:
    get_job_spec_id("43221_23.log", ".log") -> "43221_23"

    :param file:
    :param std_log:
    :return:
    """
    if std_log.__ne__("") and file[-len(std_log):].__eq__(std_log):
        job_spec_id = file[:-len(std_log)]
    else:
        job_spec_id = os.path.splitext(file)[0]
    return job_spec_id


def wrap_dict_to_table(table_dict, title="") -> Table:
    """
    Takes a dict of the format :
    {
        column1: [Header1, Header2, Header3]
        column2: [val1, val2, val3]
    }
    Why ? Because the tool tabulate took the data like this
    and this function is supposed to reduce the usage of tabulate
    without too much work
    :param table_dict:
    :param title: title of table
    :return:
    """

    if len(table_dict) == 0:
        return

    table = Table(title=title,
                  show_header=True,
                  header_style="bold magenta",
                  box=box.ASCII)
    headers = list(table_dict.keys())
    n_vals = 0
    for val in headers:
        table.add_column(val)
        if n_vals == 0:
            n_vals = len(table_dict[val])
        # Todo: could be reduce to one call
    for i in range(n_vals):
        new_list = list()
        # get the values from each column, convert to str
        for val in table_dict:
            new_list.append(str(table_dict[val][i]))
        table.add_row(*new_list)

    # rprint(table)
    return table


# in order that plotille has nothing like a int converter,
# I have to set it up manually to show the y - label in number format
def _int_formatter(val, chars, delta, left=False):
    """
    Usage of this is shown here:
    https://github.com/tammoippen/plotille/issues/11

    :param val:
    :param chars:
    :param delta:
    :param left:
    :return:
    """
    align = '<' if left else ''
    return '{:{}{}d}'.format(int(val), align, chars)


def default(log_files: list_of_logs, show_list=[]) -> log_inf_list:
    """
    Print the default output for a given list of log files

    This mode is just an easy view,
     on what the script is actually doing.

    :param log_files:
    :return: list of dicts
    """

    logging.info('Starting the default mode')

    std_log = GlobalServant.std_log
    std_err = GlobalServant.std_err
    std_out = GlobalServant.std_out

    list_of_dicts = list()
    # current_path = os.getcwd()
    # go through all given logs
    for file in track(log_files, transient=True, description="Processing..."):

        result_dict = dict()
        htcondor_log = log_to_dict(file)

        job_dict = htcondor_log[0]
        res_dict = htcondor_log[1]
        times = htcondor_log[2]

        result_dict["description"] = f"" \
            f"[green]The job procedure of : " \
            f"{file}[/green]"

        result_dict["execution-details"] = job_dict

        result_dict["times"] = times
        if not len(res_dict) == 0:  # make sure res_df is not None

            res_dict = GlobalServant.manage_thresholds(res_dict)

            result_dict["all-resources"] = res_dict

        if len(show_list) > 0:
            job_spec_id = get_job_spec_id(file, std_log)
            if 'std-err' in show_list:
                result_dict['stderr'] = htcondor_stderr(
                    job_spec_id + std_err, std_err)
            if 'std-out' in show_list:
                result_dict['stdout'] = htcondor_stdout(
                    job_spec_id + std_out, std_out)

        list_of_dicts.append(result_dict)

    if len(list_of_dicts) == 0:
        rprint("[yellow]Nothing found,"
               " please use \"man htcompact\" "
               "or \"htcompact -h\" for help[/yellow]", end="")

    return list_of_dicts


def analyse(log_files: list_of_logs, show_list=[]) -> log_inf_list:
    """

    :param log_files: list of valid HTCondor log files
    :return: list with information of each log file
    """
    logging.info('Starting the analyser mode')

    if len(log_files) == 0:
        return "No files to analyse"
    elif len(log_files) > 5 and not GlobalServant.redirecting_stdout:
        print(
            "More than five files are given, "
            "this mode is meant to be used for single job analysis.\n"
            "This will change nothing,"
            " but you should rather do it just for a file one by one")
        if not GlobalServant.reading_stdin:
            x = input("Want to continue (y/n): ")
            if x != "y":
                rprint('[red]Process stopped[/red]')
                sys.exit(0)

    result_list = list()

    std_log = GlobalServant.std_log
    std_err = GlobalServant.std_err
    std_out = GlobalServant.std_out

    # create progressbar, do not redirect output
    with Progress(transient=True, redirect_stdout=False,
                  redirect_stderr=False) as progress:

        task = progress.add_task("Analysing...", total=len(log_files))

        for file in log_files:
            progress.update(task, advance=1)
            result_dict = dict()

            logging.debug(f"Analysing the HTCondor log file: {file}")
            result_dict["description"] = f"[green]" \
                f"Job analysis of: {file}[/green]"

            job_dict, res_dict, time_dict, \
                ram_history, occurred_errors = log_to_dict(file)
            if len(job_dict) != 0:
                result_dict["execution-details"] = job_dict

            if len(time_dict) > 0:
                result_dict["times"] = time_dict

            if len(res_dict) > 0:
                result_dict["all-resources"] = \
                    GlobalServant.manage_thresholds(res_dict)

            # show HTCondor errors
            if len(occurred_errors) > 0:
                result_dict["errors"] = occurred_errors

            # managing the ram history
            if len(ram_history) > 0:
                ram = np.array(ram_history.get('Image size updates'))
                dates = np.array(ram_history.get('Dates'))

                if len(ram) > 1:

                    fig = Figure()
                    fig.width = 55
                    fig.height = 15
                    fig.set_x_limits(min_=min(dates))
                    min_ram = int(min(ram))  # raises an error if not casted
                    fig.set_y_limits(min_=min_ram)
                    fig.y_label = "Usage"
                    fig.x_label = "Time"

                    # this will use the self written function _
                    # num_formatter, to convert the y-label to int values
                    fig.register_label_formatter(float, _int_formatter)
                    fig.plot(dates, ram, lc='green', label="Continuous Graph")
                    fig.scatter(dates, ram, lc='red', label="Single Values")

                    # if redirected, the Legend is useless
                    if GlobalServant.redirecting_stdout:
                        result_dict["ram-history"] = fig.show()
                    else:
                        result_dict["ram-history"] = fig.show(legend=True)
                else:
                    result_dict["ram-history"] = f"" \
                        f"Single memory update found:\n" \
                        f"Memory usage on the {dates[0]}" \
                        f" was updatet to {ram[0]} MB"

            if len(show_list) > 0:
                job_spec_id = get_job_spec_id(file, std_log)
                if 'std-err' in show_list:
                    result_dict['stderr'] = htcondor_stderr(
                        job_spec_id + std_err,
                        std_err)
                if 'std-out' in show_list:
                    result_dict['stdout'] = htcondor_stdout(
                        job_spec_id + std_out,
                        std_out)

            result_list.append(result_dict)

    return result_list


def sort_dict_by_col(node_dict, column):
    # sorted_dict = dict.fromkeys(node_dict.keys())
    sorted_dict = {key: [] for key in node_dict.keys()}
    sort_index = list(sorted_dict.keys()).index(column)
    zip_data = zip(*node_dict.values())
    sorted_items = sorted(zip_data, key=lambda tup: tup[sort_index])
    for item in reversed(sorted_items):
        for i, key in enumerate(sorted_dict.keys()):
            sorted_dict[key].append(item[i])

    return sorted_dict


def summarize(log_files: list_of_logs) -> log_inf_list:
    """
    Summarises all used resources and the runtime in total and average
    for normal executed jobs

    Runs through the log files via the log_to_dict function

    :return:
    """

    logging.info('Starting the summarizer mode')

    valid_files = len(log_files)
    # no given files
    if valid_files == 0:
        return "No files to summarize"

    # allocated all diffrent datatypes, easier to handle
    result_dict = dict()

    aborted_files = 0
    still_running = 0
    error_reading_files = 0
    other_exception = 0
    normal_runtime = datetime.timedelta()
    host_nodes = dict()

    total_usages = np.array([0, 0, 0], dtype=float)
    total_requested = np.array([0, 0, 0], dtype=float)
    total_allocated = np.array([0, 0, 0], dtype=float)

    for file in track(log_files, transient=True, description="Summarizing..."):
        try:
            job_dict, res_dict, time_dict, _, _ = log_to_dict(file)

            # continue if Process is still running
            if job_dict['Execution details'][0].__eq__("Process is"):
                still_running += 1
                continue
            elif job_dict['Execution details'][0].__eq__("Process was"):
                aborted_files += 1
                continue
            elif job_dict['Execution details'][0].__eq__("Error"):
                error_reading_files += 1
                continue
            elif len(job_dict) == 0:
                logging.error(
                    "if this even get's printed out, more work is needed")
                rprint(f"[orange3]Process of {file} is strange, \n"
                       f"don't know how to handle this yet[/orange3]")
                other_exception += 1
                continue

            if "Total runtime" in time_dict["Dates and times"]:
                normal_runtime += time_dict['Values'][3]
            host = job_dict['Values'][2]
            if host in host_nodes:
                host_nodes[host][0] += 1
                host_nodes[host][1] += time_dict['Values'][3]
            else:
                host_nodes[host] = [1, time_dict['Values'][3]]

            total_usages += np.nan_to_num(res_dict["Usage"])
            total_requested += np.nan_to_num(res_dict["Requested"])
            total_allocated += np.nan_to_num(res_dict["Allocated"])

        # Error ocurres when Job was aborted
        except ValueError or KeyError as err:
            logging.exception(err)
            rprint(f"[red]Error with summarizing: {file}[/red]")
            continue
        except TypeError as err:
            logging.exception(err)
            rprint(f"[red]{err.__class__.__name__}: {err}[/red]")
            sys.exit(3)

    # calc difference of successful executed jobs
    n = valid_files - aborted_files - still_running\
        - other_exception - error_reading_files

    average_runtime = normal_runtime / n if n != 0 else normal_runtime
    average_runtime = datetime.timedelta(days=average_runtime.days,
                                         seconds=average_runtime.seconds)

    exec_dict = {
        "Job types": ["normal executed jobs"],
        "Occurrence": [n]
    }
    if aborted_files > 0:
        exec_dict["Job types"].append("Aborted jobs")
        exec_dict["Occurrence"].append(aborted_files)
    if still_running > 0:
        exec_dict["Job types"].append("Still running jobs")
        exec_dict["Occurrence"].append(still_running)
    if error_reading_files > 0:
        exec_dict["Job types"].append("Error while reading")
        exec_dict["Occurrence"].append(error_reading_files)
    if other_exception > 0:
        exec_dict["Job types"].append("Other exceptions")
        exec_dict["Occurrence"].append(other_exception)

    result_dict["execution-details"] = \
        sort_dict_by_col(exec_dict, "Occurrence")

    result_dict["description"] = "The following data only implies" \
                                 " on sucessful executed jobs"

    # do not even try futher if the only files
    # given have been aborted, are still running etc.
    if n == 0:
        return [result_dict]

    create_desc = "The following data only implies on sucessful executed jobs"
    if aborted_files > 0 or still_running > 0 \
            or other_exception > 0 or error_reading_files:
        create_desc += "\n[light_grey]" \
            "Use the analysed-summary mode" \
            " for more details about the other jobs" \
            "[/light_grey]"

    result_dict["summation-description"] = create_desc

    time_desc_list = list()
    time_value_list = list()
    if normal_runtime != datetime.timedelta(0, 0, 0):
        time_desc_list.append("Total runtime")
        time_value_list.append(normal_runtime)
    if average_runtime:
        time_desc_list.append("Average runtime")
        time_value_list.append(average_runtime)

    result_dict["times"] = {
        "Times": time_desc_list,
        "Values": time_value_list
    }

    if n != 0:  # do nothing, if all valid jobs were aborted

        average_dict = {
            "Resources": ['Average Cpu', 'Average Disk (KB)',
                          'Average Memory (MB)'],
            "Usage": np.round(total_usages / n, 4),
            "Requested": np.round(total_requested / n, 2),
            "Allocated": np.round(total_allocated / n, 2)

        }

        average_dict = GlobalServant.manage_thresholds(average_dict)

        result_dict["all-resources"] = average_dict

    if len(host_nodes) > 0:

        executed_jobs = list()
        runtime_per_node = list()
        for val in host_nodes.values():
            executed_jobs.append(val[0])
            average_job_duration = val[1] / val[0]
            runtime_per_node.append(
                datetime.timedelta(average_job_duration.days,
                                   average_job_duration.seconds))

        cpu_dict = {
            "Host Nodes": list(host_nodes.keys()),
            "Executed Jobs": executed_jobs,
            "Average job duration": runtime_per_node
        }

        result_dict["host-nodes"] = sort_dict_by_col(cpu_dict, "Executed Jobs")

    return [result_dict]


def analysed_summary(log_files: list_of_logs) -> log_inf_list:
    """
        analyse the summarized log files,
        this is meant to give the ultimate output
        about every single log event in average etc.

        Runs through the log files via the log_to_dict function

        :return: string
        """

    logging.info('Starting the analysed summary mode')

    valid_files = len(log_files)
    # no given files
    if valid_files == 0:
        return "No files for the analysed summary"

    # fill this dict with information by the execution type of the jobs
    all_files = dict()
    list_of_gpu_names = list()  # list of gpus found
    occurrence_dict = dict()

    for file in track(log_files, transient=True, description="Summarizing..."):

        (job_dict, res_dict, time_dict,
         ram_history, occurred_errors) = log_to_dict(file)

        if len(occurred_errors) > 0:
            create_file_list = list()
            for i in range(len(occurred_errors["Event Number"])):
                create_file_list.append(file)
            occurred_errors['File'] = create_file_list

        refactor_job_dict = dict(
            zip(job_dict["Execution details"], job_dict["Values"]))
        job_keys = list(refactor_job_dict.keys())
        to_host = None
        if "Executing on" in job_keys:
            to_host = refactor_job_dict["Executing on"]

        term_type = job_dict["Values"][0]

        # if time dict exists
        time_keys = list()
        if time_dict:
            refactor_time_dict = dict(
                zip(time_dict["Dates and times"], time_dict["Values"]))
            time_keys = list(refactor_time_dict.keys())
        if "Waiting time" in time_keys:
            waiting_time = refactor_time_dict["Waiting time"]
        else:
            waiting_time = datetime.timedelta()
        if "Execution runtime" in time_keys:
            runtime = refactor_time_dict["Execution runtime"]
        else:
            runtime = datetime.timedelta()
        if "Total runtime" in time_keys:
            total_time = refactor_time_dict["Total runtime"]
        else:
            total_time = datetime.timedelta()

        try:
            if term_type in all_files:
                # logging.debug(all_files[termination_type])
                all_files[term_type][0] += 1  # count number
                all_files[term_type][1] += waiting_time
                all_files[term_type][2] += runtime
                all_files[term_type][3] += total_time

                # add errors
                if len(occurred_errors) > 0:
                    for key in occurred_errors.keys():
                        # extend if already existent
                        if key in all_files[term_type][6].keys():
                            all_files[term_type][6][key].extend(
                                occurred_errors[key])
                        else:
                            all_files[term_type][6] = occurred_errors

                if not len(all_files[term_type][4]) == 0:
                    # add usages

                    all_files[term_type][4]["Usage"] += np.nan_to_num(
                        res_dict["Usage"])
                    # add requested
                    all_files[term_type][4][
                        "Requested"] += np.nan_to_num(res_dict["Requested"])
                    # allocated
                    all_files[term_type][4][
                        "Allocated"] += np.nan_to_num(res_dict["Allocated"])

                # add cpu
                if to_host is not None:
                    # cpu known ???
                    if to_host in all_files[term_type][5].keys():
                        all_files[term_type][5][to_host][0] += 1
                        all_files[term_type][5][to_host][
                            1] += total_time
                    else:
                        all_files[term_type][5][to_host] = [1, total_time]
                elif "Submitted from" in job_dict["Execution details"]:
                    # other waiting jobs ???
                    if 'Waiting for execution' in \
                            all_files[term_type][5].keys():
                        all_files[term_type][5][
                            'Waiting for execution'][0] += 1
                        all_files[term_type][5][
                            'Waiting for execution'][1] += total_time
                    elif "Aborted before execution" in \
                            all_files[term_type][5].keys():
                        all_files[term_type][5][
                            'Aborted before execution'][0] += 1
                        all_files[term_type][5][
                            'Aborted before execution'][1] += total_time
                    else:
                        count_host_nodes = dict()
                        if "Aborted" in term_type:
                            count_host_nodes['Aborted before'
                                             ' execution'] = [1, total_time]
                        else:
                            count_host_nodes['Waiting for'
                                             ' execution'] = [1, total_time]
                        all_files[term_type][5] = count_host_nodes
                else:
                    # other aborted before submission jobs ???
                    if 'Aborted before submission' in \
                            all_files[term_type][5].keys():
                        all_files[term_type][5][
                            'Aborted before submission'][0] += 1
                        all_files[term_type][5][
                            'Aborted before submission'][1] += total_time
                    else:
                        count_host_nodes = dict()
                        count_host_nodes['Aborted before '
                                         'submission'] = [1, total_time]
                        all_files[term_type][5] = count_host_nodes

            # else new entry
            else:
                # if host exists
                if "Executing on" in job_dict["Execution details"]:
                    # to_host = job_dict["Values"][2]
                    count_host_nodes = dict()
                    count_host_nodes[to_host] = [1, total_time]
                # else if still waiting
                elif "Submitted from" in job_dict["Execution details"]:
                    count_host_nodes = dict()
                    if "Aborted" in term_type:
                        count_host_nodes['Aborted before'
                                         ' execution'] = [1, total_time]
                    else:
                        count_host_nodes['Waiting for'
                                         ' execution'] = [1, total_time]
                # else aborted before submission ?
                else:
                    count_host_nodes = dict()
                    count_host_nodes['Aborted before'
                                     ' submission'] = [1, total_time]

                # convert nan values to 0
                if len(res_dict) > 0:
                    res_dict["Usage"] = np.nan_to_num(res_dict["Usage"])
                    res_dict["Requested"] = np.nan_to_num(
                        res_dict["Requested"])
                    res_dict["Allocated"] = np.nan_to_num(
                        res_dict["Allocated"])

                all_files[term_type] = [1,
                                        waiting_time,
                                        runtime,
                                        total_time,
                                        res_dict,
                                        count_host_nodes,
                                        occurred_errors]

        # Error ocurres when Job was aborted
        except ValueError or KeyError as err:
            logging.exception(err)
            logging.debug(f"[red]Error with summarizing: {file}[/red]")
            rprint(f"[red]Error with summarizing: {file}[/red]")
            continue
        except TypeError as err:
            logging.exception(err)
            logging.debug(f"[red]Error with summarizing: {file}[/red]")
            rprint(f"[red] {err}[/red]")
            sys.exit(3)

    # Now put everything together
    result_list = list()
    for term_state in all_files:
        term_info = all_files[term_state]
        result_dict = dict()

        # differentiate between terminated and running processes
        if "Error while reading" in term_state:
            result_dict["description"] = "" \
                "##################################################\n" \
                "## All files, that caused an [red]" \
                "error while reading[/red]\n" \
                "##################################################"
        elif term_state not in ["Waiting", "Executing"]:
            result_dict["description"] = f"" \
                f"##################################################\n" \
                f"## All files with the termination state: {term_state}\n" \
                f"##################################################"
        else:
            result_dict["description"] = f"" \
                f"###########################################\n" \
                f"## All files, that are currently {term_state}\n" \
                f"###########################################"

        n = int(term_info[0])
        occurrence_dict[term_state] = str(n)

        times = np.array([term_info[1], term_info[2], term_info[3]])
        av_times = times / n
        format_av_times = [
            datetime.timedelta(days=time.days, seconds=time.seconds) for time
            in av_times]

        time_dict = {
            "Times": ["Waiting time", "Runtime", "Total"],
            "Average": format_av_times,
            "Total": times
        }

        result_dict["times"] = time_dict

        if not len(term_info[4]) == 0:
            total_resources_dict = term_info[4]
            avg_dict = {
                'Resources': ['Average Cpu', ' Average Disk (KB)',
                              'Average Allocated'],
                'Usage': np.round(
                    np.array(total_resources_dict['Usage']) / term_info[0],
                    4).tolist(),
                'Requested': np.round(
                    np.array(total_resources_dict['Requested']) / term_info[0],
                    2).tolist(),
                'Allocated': np.round(
                    np.array(total_resources_dict['Allocated']) / term_info[0],
                    2).tolist()
            }
            if 'Assigned' in total_resources_dict.keys():
                avg_dict['Resources'].append('Gpu')
                avg_dict['Assigned'] = ['', '', '',
                                        ", ".join(list_of_gpu_names)]

            avg_dict = GlobalServant.manage_thresholds(avg_dict)
            result_dict["all-resources"] = avg_dict

        executed_jobs = list()
        runtime_per_node = list()
        for val in term_info[5].values():
            executed_jobs.append(val[0])
            average_job_duration = val[1] / val[0]
            runtime_per_node.append(
                datetime.timedelta(average_job_duration.days,
                                   average_job_duration.seconds))

        host_nodes_dict = {
            "Host Nodes": list(term_info[5].keys()),
            "Executed Jobs": executed_jobs,
            "Average job duration": runtime_per_node
        }

        result_dict["host-nodes"] = \
            sort_dict_by_col(host_nodes_dict, "Executed Jobs")

        if len(term_info[6]) > 0:
            temp_err = term_info[6]
            del temp_err["Reason"]  # remove reason, cause thats just too much
            result_dict["errors"] = temp_err

        result_list.append(result_dict)

    new_occ = {
        "Termination type": list(occurrence_dict.keys()),
        "Appearance": list(occurrence_dict.values())
    }
    sorted_occ = sort_dict_by_col(new_occ, "Appearance")

    result_list.insert(0, {"execution-details": sorted_occ})

    return result_list


# search in the files for the keywords
def filter_for(log_files: list_of_logs,
               keywords: list,
               extend=False,
               mode=None,
               show_list=[]) -> log_inf_list:
    """
    Filter for a list of keywords, which can be extended
    and print out every file which matches the pattern (not case sensitive)
    The filtered files can be analysed summarise, etc afterwards,
    else this function will return None

    :param log_files:
    :param keywords:
    :param extend:
    :return:
        list with dicts depending on the used mode,
        to forward the filtered files,

        None if no forwarding is set
    """
    logging.info('Starting the filter mode')

    # if the keywords are given as a string, try to create a list
    if isinstance(keywords, list):
        keyword_list = keywords
    else:
        logging.debug(
            f"Filter mode only accepts a string"
            f" or list with keywords, not {keywords}")
        raise_type_error("Expecting a list or a string")

    # if extend is set, keywords like err will
    # also look for keywords like warn exception, aborted, etc.
    if extend:  # apply some rules
        # the error list
        err_list = ["err", "warn", "exception", "aborted", "abortion",
                    "abnormal", "fatal"]

        # remove keyword if already in err_list
        for i in range(len(keyword_list)):
            if (keyword_list[i]).lower() in err_list:
                keyword_list.remove(keyword_list[i])

        keyword_list.extend(err_list)  # extend search

        rprint("[green]Keyword List was extended,"
               " now search for these keywords:[/green]",
               keyword_list)
    else:
        rprint("[green]Search for these keywords:[/green]", keyword_list)

    if len(keyword_list) == 1 and keyword_list[0] == "":
        logging.debug("Empty filter, don't know what to do")
        return "[yellow]" \
            "Don't know what to do with an empty filter,\n" \
            "if you activate the filter mode in the config file, \n" \
            "please add a [filter] section with the filter" \
            "_keywords = your_filter[/yellow]"

    logging.debug(f"These are the keywords to look for: {keyword_list}")

    # now search
    found_at_least_one = False
    found_logs = []
    for file in track(log_files, transient=True, description="Filtering..."):
        found = False
        with open(file, "r") as read_file:
            for line in read_file:
                for keyword in keyword_list:
                    if re.search(keyword.lower(), line.lower()):
                        if not found_at_least_one:
                            print("Matches:")
                        rprint(f"[grey74]{keyword} in:\t{file}[/grey74] ")
                        found = True
                        found_at_least_one = True
                        break
                if found:
                    found_logs.append(file)
                    break

    return_dicts = None
    if not found_at_least_one:
        rprint("[red]Unable to find these keywords:[/red]", keyword_list)
        rprint("[red]maybe try again with similar expressions[/red]")

    elif mode is not None:
        print(f"Total count: {len(found_logs)}")
        if mode.__eq__("default"):
            return_dicts = default(found_logs, show_list)
        elif mode.__eq__("analysed-summary"):
            rprint("[magenta]Give an analysed summary"
                   " for these files[/magenta]")
            return_dicts = analysed_summary(found_logs)
        elif mode.__eq__("summarize"):
            rprint("[magenta]Summarize these files[/magenta]")
            return_dicts = summarize(found_logs)
        elif mode.__eq__("analyse"):
            rprint("[magenta]Analyse these files[/magenta]")
            return_dicts = analyse(found_logs, show_list)
    # if not reading from stdin or redirected
    elif not GlobalServant.reading_stdin \
            and not GlobalServant.redirecting_stdout:
        rprint("[blue]Want do do more?[/blue]")
        x = input(
            "default(d), summarize(s), analyse(a),"
            " analysed summary(as), exit(e): ")
        if x == "d":
            return_dicts = default(found_logs, show_list)
        elif x == "s":
            return_dicts = summarize(found_logs)
        elif x == "a":
            return_dicts = analyse(found_logs, show_list)
        elif x == "as":
            return_dicts = analysed_summary(found_logs)
        elif x == "e":
            sys.exit(0)
        else:
            print('Not a valid argument, quitting ...')
            sys.exit(0)

    return return_dicts


class LogValidator:
    """
    Create a HTCondor Joblog Validator

    Validation is visual represented by rich.progress


    """

    def __init__(self,
                 std_log="",
                 std_err=".err",
                 std_out=".out",
                 recursive=False):
        self.std_log = std_log
        self.std_err = std_err
        self.std_out = std_out
        self.recursive = recursive

    def validate_file(self, file) -> bool:
        """
         Validate a single HTCondor joblog file.

        :param file: HTCondor log file
        :return: True when valid else False
        """

        # if not ending with stdout
        if self.std_log.__ne__("") and not file.endswith(self.std_log):
            return False

        # does also not end with sterr and stdout suffix
        if self.std_err.__ne__("") and file.endswith(self.std_err):
            return False
        if self.std_out.__ne__("") and file.endswith(self.std_out):
            return False

        if os.path.getsize(file) == 0:  # file is empty
            logging.debug(f"{file} is empty")
            return False

        try:
            with open(file, "r") as read_file:

                if re.match(r"[0-9]{3} \([0-9]+.[0-9]+.[0-9]{3}\)",
                            read_file.readline()):

                    return True
                else:
                    return False
        except Exception:
            return False

    def validate_dir(self, directory, progress_details=None) -> list_of_logs:
        """
        Validate all files inside the given directory.

        :param directory: path to directory with logs
        :param progress_details: Quadrupel (progress, task, total, advance)
        :return:
        """
        valid_dir_files = list()
        file_dir = os.listdir(directory)
        # progress bar given
        if progress_details is not None:
            progress = progress_details[0]
            task = progress_details[1]
            total = progress_details[2] + len(file_dir) - 1  # minus dir
            advance = progress_details[3]
            progress.console.print(
                f"[light_coral]Search: {directory} "
                f"for valid log files[/light_coral]")
        for file in file_dir:
            if progress_details is not None:
                progress.update(task, total=total, advance=advance)

            # ignore hidden files or python modules
            if file.startswith(".") or file.startswith("__"):
                continue

            # if std_log is set, ignore other log files
            if self.std_log.__ne__("") and not file.endswith(self.std_log):
                logging.debug("Ignoring this file, " + file +
                              ", because std-log is: " + self.std_log)
                continue

            # backslash handling
            if directory.endswith('/'):
                file_path = directory + file
            else:
                file_path = directory + '/' + file

            if os.path.isfile(file_path):
                if self.validate_file(file_path):
                    valid_dir_files.append(file_path)

            elif self.recursive:
                # total = total -1
                # extend files by searching through this directory
                valid_dir_files.extend(
                    self.validate_dir(file_path, progress_details))
            else:
                logging.debug(
                    f"Found subfolder: "
                    f"{file_path}"
                    f", it will be ignored")

        return valid_dir_files

    def htcompact_validation(self, file_list: list_of_logs) -> list_of_logs:
        """
        Function designed especially for this script.
        Filters given files for valid HTCondor log files,
        the process will be visual presented by rich.progress

        :param file_list: list of HTCondor logs, that need to be validated
        :return: list with valid HTCondor log files
        """
        valid_files = list()
        total = len(file_list)

        logging.info('Validate given log files')
        with Progress(transient=True) as progress:

            task = progress.add_task("Validating...", total=total, start=False)
            for arg in file_list:

                path = os.getcwd()  # mainly search in cwd
                logs_path = path + "/" + arg  # absolute path

                working_dir_path = ""
                working_file_path = ""

                if os.path.isdir(arg):
                    working_dir_path = arg
                elif os.path.isdir(logs_path):
                    working_dir_path = logs_path

                elif os.path.isfile(arg):
                    working_file_path = arg
                elif os.path.isfile(logs_path):
                    working_file_path = logs_path
                # check if only the id was given
                # and resolve it with the std_log specification
                elif os.path.isfile(arg + self.std_log):
                    working_file_path = arg + self.std_log
                elif os.path.isfile(logs_path + self.std_log):
                    working_file_path = logs_path + self.std_log

                # if path is a directory
                if working_dir_path.__ne__(""):

                    progress_details = progress, task, total, 1
                    valid_dir_files = self.validate_dir(working_dir_path,
                                                        progress_details)
                    valid_files.extend(valid_dir_files)

                # else if path "might" be a valid HTCondor file
                elif working_file_path.__ne__(""):
                    progress.update(task, advance=1)

                    if self.validate_file(working_file_path):
                        valid_files.append(working_file_path)
                    else:
                        progress.console.print(
                            f"[yellow]The given file {working_file_path} "
                            f"is not a valid HTCondor log file[/yellow]")

                else:
                    logging.error(f"The given file: {arg} does not exist")
                    rprint(f"[red]The given file: {arg} does not exist[/red]")

        return valid_files


def setup_logging_tool(log_file=None, verbose_mode=False):
    """
        Set up the logging device,
        to generate a log file, with --generate-log-file
        or to print more descriptive output with the verbose mode to stdout

        both modes are compatible together
    :return:
    """

    # disable the loggeing tool by default
    logging.getLogger().disabled = True

    # I don't know why a root handler is already set,
    # but we have to remove him in order
    # to get just the output of our own handler
    if len(logging.root.handlers) == 1:
        default_handler = logging.root.handlers[0]
        logging.root.removeHandler(default_handler)

    # if logging tool is set to use
    if log_file is not None:
        # activate logger if not already activated
        logging.getLogger().disabled = False

        # more specific view into the script itself
        logging_file_format = '%(asctime)s - [%(funcName)s:%(lineno)d]' \
                              ' %(levelname)s : %(message)s'
        file_formatter = logging.Formatter(logging_file_format)

        handler = RotatingFileHandler(log_file, maxBytes=1000000,
                                      backupCount=1)
        handler.setLevel(logging.DEBUG)
        handler.setFormatter(file_formatter)

        log = logging.getLogger()
        log.setLevel(logging.DEBUG)
        log.addHandler(handler)

    if verbose_mode:
        # activate logger if not already activated
        logging.getLogger().disabled = False

        logging_stdout_format = '%(asctime)s - %(levelname)s: %(message)s'
        stdout_formatter = logging.Formatter(logging_stdout_format)

        stdout_handler = logging.StreamHandler(sys.stdout)
        stdout_handler.setLevel(logging.DEBUG)
        stdout_handler.setFormatter(stdout_formatter)
        log = logging.getLogger()
        log.setLevel(logging.DEBUG)
        log.addHandler(stdout_handler)


def print_results(log_files: list_of_logs,
                  mode: str,
                  ignore_list=list,
                  show_list=list,
                  filter_keywords=list,
                  filter_extended=False,
                  **kwargs) -> str:
    """
    Resolves the given log fiels by the specified mode and
    prints out the result.
    Before doing that it will remove sections specified by the ignore_list
    and show information specified by show_list
    It is also possible to filter the files by specified filter keywords

    :param log_files:
    :param mode:
    :param ignore_list:
    :param show_list:
    :param filter_keywords:
    :param filter_extended:
    :param kwargs:
    :return:
    """

    if len(filter_keywords) > 0:
        results = filter_for(log_files,
                             keywords=filter_keywords,
                             extend=filter_extended,
                             mode=mode,
                             show_list=show_list)
    elif mode.__eq__("default"):
        results = default(log_files, show_list)  # force default with -d
    elif mode.__eq__("analysed-summary"):
        results = analysed_summary(log_files)  # analysed summary ?
    elif mode.__eq__("summarize"):
        results = summarize(log_files)  # summarize information
    elif mode.__eq__("analyse"):
        results = analyse(log_files, show_list)  # analyse the given log_files
    else:
        results = default(log_files, show_list)
        # anyways try to print default output

    # This can happen, when for example the filter mode is not forwarded
    if results is None:
        sys.exit(0)

    work_with = results
    # convert result to list, if given as dict
    if isinstance(results, dict):
        work_with = [results]

    # check for ignore values
    for mystery in work_with:

        for i in mystery:
            if mystery[i] is None:
                logging.debug("This musst be fixed,"
                              " mystery['" + i + "'] is None.")
                rprint("[red]NoneType object found,"
                       " this should not happen[/red]")

        if "description" in mystery:
            rprint(mystery["description"])

        if "execution-details" in mystery:
            if "execution-details" in ignore_list:
                del mystery["execution-details"]
            else:
                table = wrap_dict_to_table(mystery["execution-details"])
                rprint(table)

        if "times" in mystery:
            if "times" in ignore_list:
                del mystery["times"]
            else:
                table = wrap_dict_to_table(mystery["times"])
                rprint(table)

        if "all-resources" in mystery:
            if "all-resources" in ignore_list:
                del mystery["all-resources"]
            else:
                if "used-resources" in ignore_list:
                    del mystery["all-resources"]["Usage"]
                if "requested-resources" in ignore_list:
                    del mystery["all-resources"]["Requested"]
                if "allocated-resources" in ignore_list:
                    del mystery["all-resources"]["Allocated"]

                table = wrap_dict_to_table(mystery["all-resources"])
                rprint(table)

        if "ram-history" in mystery:
            if "ram-history" in ignore_list:
                del mystery["ram-history"]
            elif mystery["ram-history"] is not None:
                print(mystery["ram-history"])

        if "errors" in mystery:
            if "errors" in ignore_list:
                del mystery["errors"]
            elif mystery["errors"] is not None:
                table = wrap_dict_to_table(mystery["errors"],
                                           "Occurred HTCondor errors")
                rprint(table)

        if "host-nodes" in mystery:
            if "host-nodes" in ignore_list:
                del mystery["host-nodes"]
            elif mystery["host-nodes"] is not None:
                table = wrap_dict_to_table(mystery["host-nodes"])
                rprint(table)

        #### Show more section ####
        if "stdout" in mystery:
            rprint("\n[bold cyan]Standard output (std-out):[/bold cyan]")
            rprint(mystery["stdout"])

        if "stderr" in mystery:
            rprint("\n[bold cyan]Standard errors (std-err):[/bold cyan]")
            rprint(mystery["stderr"])

        print()


def run(commandline_args):
    """
    Run this script

    :return:
    """
    # before running make sure Global Parameters are set to default
    GlobalServant.reset()

    if not isinstance(commandline_args, list):
        commandline_args = commandline_args.split()

    try:
        start = datetime.datetime.now()  # start date for runtime

        # initialize()  # initialize global parameters

        GlobalServant.check_for_redirection()
        # if exit parameters are given that will interrupt this script,
        # catch them here so the config won't be unnecessary loaded
        param_dict = manage_params(commandline_args)

        setup_logging_tool(param_dict["generate_log_file"],
                           param_dict["verbose"])

        logging.debug("-------Start of htcompact scipt-------")

        if param_dict["verbose"]:
            logging.info('Verbose mode turned on')

        if GlobalServant.reading_stdin:
            logging.debug("Reading from stdin")
        if GlobalServant.redirecting_stdout:
            logging.debug("Output is getting redirected")

        Validator = LogValidator(GlobalServant.std_log,
                                 GlobalServant.std_err,
                                 GlobalServant.std_out,
                                 param_dict["recursive"])

        valid_files = Validator.htcompact_validation(param_dict["files"])

        rprint(f"[green]{len(valid_files)}"
               f" valid log file(s)[/green]\n")

        if len(valid_files) == 0:
            rprint("[red]No valid HTCondor log files found[/red]")
            sys.exit(1)

        print_results(log_files=valid_files, **param_dict)

        end = datetime.datetime.now()  # end date for runtime

        logging.debug(f"Runtime: {end - start}")  # runtime of this script

        logging.debug("-------End of htcompact script-------")

        sys.exit(0)

    except TypeError as err:
        logging.exception(err)
        rprint(f"[red]{err.__class__.__name__}: {err}[/red]")
        sys.exit(3)

    except KeyboardInterrupt:
        logging.info("Script was interrupted by the user")
        print("Script was interrupted")
        sys.exit(4)


if __name__ == "__main__":
    """
    This is the main function,
    which runs the script, if not imported as a module

    :return: exit status 0-4
    """
    run(sys.argv[1:])
